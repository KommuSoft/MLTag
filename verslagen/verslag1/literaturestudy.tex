\section{Literature study}
After a first exploration of some literature, we concluded there are two main strategies to solve this sort of problem. The first is using is transforming the multi-label classification problem into one or multiple single-label classification problems. Another way is to adapt a single-label algorithm to support the multi-label problem. Both approaches use single-label classification problems as a basis, so we had to study these as well.
\subparagraph{}
\cite{Tsoumakas07multi-labelclassification:} gives a general overview of the problem and suggests five transforming methods: named generic PT1 to PT5. These transforming methods vary from assigning only one label over ignoring cases with more labels to expanding the labelset to a cartesian product or even a powerset.
\subparagraph{}
Adaptive algorithms described in \cite{Tsoumakas07multi-labelclassification:} use for instance the C4.5 decision tree algorithm but modify the entropy function. Another method is making extensions on the AdaBoost learning algorithm. Modifying the $kNN$\footnote{$k$-Nearest Neighbours.}. An import statement the authors of \cite{Tsoumakas07multi-labelclassification:} make is that fundamentally even most adaptive algorithms are problem transformations. Instead of classifying the sample with all the labels at once, the system is queried with the sample and a specific label, the system needs to decide whether the label is relevant.
\paragraph{}
These algorithms use distance functions that we need to define based on characteristics of the samples, the todo sentence in our case. \cite{gjorgjioskicomparison} compares some distance measures in the context of multi-label classification problem.
\paragraph{}
Another problem that arises with the previous is what will be the characteristics of the samples. \cite{jing2006ontology} specifically studies text how to couple text samples with distance measures, and so the characteristics.
\paragraph{}
Measuring the performance of a system is an important isue. Most of the papers \cite{KTV08, park2008dimension, Tsoumakas07multi-labelclassification:} give a number of evaluation metrics including: Hamming loss, accuracy, precision, recall and F-measure. Furthermore they provide metrics for the data set as well: label cardinality and label density.